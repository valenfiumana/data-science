## Ejercicio 1: Reducción de dimensionalidad
Es este ejercicio vamos a trabajar con los datasets que ya usamos anteiormente de medidas de personas del ejército de EEUU ansurMen.csv y ansurMen.csv.

1. Carguen los dos csvs en dos dataframes distintos de pandas. Agréguenle a cada uno una nueva columna 'SEXO' que tenga los valores 'H' y 'M', según corresponda, para poder identificar de qué dataset vino cada persona. Luego unan los dos datasets en uno nuevo usando la función de pandas pd.concat([df1, df2]).

2. Definan un nuevo dataframe de variables sólo numéricas a partir del anterior, descartando las columnas 'SEXO' y 'SUBJECT_NUMBER' (¿tiene sentido quedarse con esta última columna?). Luego apliquenle el StandardScaler de sklearn a este nuevo dataframe, y hagan una reducción dimensional usando PCA. ¿Con cuántas componentes necesito quedarme para explicar el 95% de la varianza de los datos?

3. Ahora hagan otro PCA, pero quedándose sólo con 2 componentes, y hagan un scatterplot de los datos. ¿Qué es lo que se ve? Traten de pintar los puntos usando la columna categórica "SEXO" que tiene el dataset original.

4. (Opcional). Ahora hagan un PCA con un número reducido de componentes (digamos 8), y luego apliquen un TSNE con 2 componentes. Grafiquen los resultados cómo hicieron en el punto anterior. ¿Qué se ve ahora? Pueden jugar con el número de componentes del PCA, o sólo hacer TSNE, y ver las diferencias.

[Solución](./03/dim_red.py)

## Ejercicio 2: Preprocesamiento
En este ejercico vamos a trabajar con un dataset bastante problemático: el dataset de arbolado en calles de CABA, arbolado-publico-lineal-2017-2018.csv). El mismo el similar al de árboles en parques que ya hemos usado, pero tiene bastantes más registros.

1. Cargando los datos. Importen este nuevo dataset usando pandas. Van a notar que les da una advertencia (warning) porque hay algunas columnas con tipos mezclados. Por ahora ignorenlo.

    Para ahorrarnos trabajo, definan un nuevo DataFrame usando solo las columnas ['nro_registro', 'nombre_cientifico', 'estado_plantera', 'ubicacion_plantera', 'nivel_plantera', 'diametro_altura_pecho', 'altura_arbol'].

2. Limpieza de datos (I). Analicen los valores únicos que pueden tomar las columnas 'estado_plantera', 'ubicacion_plantera' y 'nivel_plantera'. ¿Qué es lo que ven?

    Para las tres columnas, unifiquen los valores que pertecen a una misma catgoría.

3. Limpieza de datos (II). Hagan histogramas de los valores de las variables 'diametro_altura_pecho' y 'altura_arbol'.

    A primera vista no parece haber nada raro, pero fijense que para el diámetro (que está medido en cm) hay muchos datos con valor 0 (pueden usar el método value_counts()). Si bien podría haber árboles con menos de 1 cm de diámetro, la cantidad de los mismos nos hace sospechar que en gran parte de los casos se trata de un error.

    Eliminen las filas con diámetro 0, o al menos por ahora reemplacen el valor por nan.

4. Datos faltantes. Analicen la cantidad de datos faltantes en cada columna y decidan qué hacer con ellos (descartarlos, crear una nueva categoría en las variables categóricas, reemplazarla por promedio/mediana en las numéricas, etc.)

5. Variables categóricas. Apliquen el método de One-Hot Encoding a alguna de las variables categóricas del dataset. ¿De qué va a depender la cantidad de componentes de los vectores resultantes?

[Solución](./03/preprocessing.py)